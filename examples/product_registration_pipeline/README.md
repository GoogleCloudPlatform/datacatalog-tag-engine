## Product Registration Pipeline
This folder contains a workflow that creates the data discovery and data protection tags for a data product. The workflow is triggered via a Data Catalog tag update event in which the status field of a data product tag is updated to "Pending". The workflow runs a Cloud Function, which in turn calls Tag Engine to generate the required tags on the data assets of the product. Once all the tags have been generated, the workflow updates the status of the data product tag to "Review". This signals to the Data Steward to review the accuracy of the generated tags before granting users access to the metadata via the BQ Metadata Viewer role.  

### Dependencies

In order to implement this workflow, you must have a running instance of Tag Engine. Moreover, Tag Engine must be running on version 1.0.5 or later. 

If you are new to Tag Engine, start with [this tutorial](https://cloud.google.com/architecture/tag-engine-and-data-catalog). You can follow [this README](https://github.com/GoogleCloudPlatform/datacatalog-tag-engine/blob/main/README.md) to deploy Tag Engine on Google Cloud Platform. 


### Deployment Procedure

The following procedure assumes that you have a running instance of Tag Engine. 


#### Step 1: Create the tag templates

This workflow makes use of four tag templates:
* [data product](https://github.com/GoogleCloudPlatform/datacatalog-templates/blob/master/data_product.yaml)
* [data resource](https://github.com/GoogleCloudPlatform/datacatalog-templates/blob/master/data_resource.yaml)
* [data sensitivity](https://github.com/GoogleCloudPlatform/datacatalog-templates/blob/master/data_sensitivity.yaml)
* [data standardization](https://github.com/GoogleCloudPlatform/datacatalog-templates/blob/master/data_standardization.yaml)

To create them, clone the [tag template repository](https://github.com/GoogleCloudPlatform/datacatalog-templates.git) and run the [create_template.py](https://github.com/GoogleCloudPlatform/datacatalog-templates/blob/master/create_template.py) script as follows:

```
git clone https://github.com/GoogleCloudPlatform/datacatalog-templates.git

export PROJECT='my-project-id'
export REGION='us-central1'

python create_template.py $PROJECT $REGION data_product.yaml
python create_template.py $PROJECT $REGION data_resource.yaml
python create_template.py $PROJECT $REGION data_sensitivity.yaml
python create_template.py $PROJECT $REGION data_standardization.yaml
```

#### Step 2: Update Tag Engine configs and upload them to Cloud Storage

The subfolders _CRM_, _Customer_Churn_, Trade_Transactions, and Finwire_Archives_ represent sample data products and each one contains the configuration files that are used by Tag Engine to create and populate the tags. 

The configuration files are:
* _data_product.yaml_
* _data_resource.yaml_
* _data_sensitivity.yaml_
* _data_standardization.yaml_

These example products are based on the [TPCDI dataset](https://www.tpc.org/tpc_documents_current_versions/pdf/tpc-di_v1.1.0.pdf). You can either download the TPCDI data and load it into BQ or you can use a different dataset. 

Either way, you'll want to update the references to the GCP project, BQ dataset, Policy Tag taxonomy in the configuration files. If you're not using the TPCDI dataset, you'll also want to update the values of certain fields so that they make sense for your data. For example, _data_product.yaml_ contains fields like _data_domain_ and _data_subdomain_. And _data_standardization.yaml_ contains _query_expression and _included_columns_query_ which need to be updated to reflect your tables. 

Once you have made the changes to the config files, upload them into a GCS bucket. 


#### Step 3: Create the policy tag taxonomy and policy tag labels

```
export TAXONOMY_NAME='sensitive_categories'
export POLICY_TAG_LABELS='Personal_Information,Personal_Identifiable_Information,Sensitive_Personal_Information,Sensitive_Personal_Identifiable_Information'
python datacatalog-tag-engine/helper_functions/create_policy_tag_taxonomy.py $PROJECT $REGION $TAXONOMY_NAME $POLICY_TAG_LABELS
```

#### Step 4: Create the Infotype mapping tables

Create two infotype mapping tables in BigQuery. The first maps a grouping of infotypes associated with a table column to a notable infotype. The second one maps the set of notable infotypes to a classification category. 

```
bq mk --project_id=$PROJECT â€“-location=$REGION reference
cat datacatalog-tag-engine/helper_functions/create_classification_tables.sql | bq query --use_legacy_sql=false 
```

#### Step 5: Scan your data assets with DLP (Data Loss Prevention)

```
export REGION='us-central1'
export INSPECT_PROJECT='my-project-id'
export INSPECT_DATASETS='finwire,oltp,crm,sales,hr'
export RESULT_PROJECT='my-project-id'
export RESULT_DATASETS='dlp_finwire,dlp_oltp,dlp_crm,dlp_sales,dlp_hr'
python datacatalog-tag-engine/helper_functions/inspect_datasets.py $REGION $INSPECT_PROJECT $INSPECT_DATASETS $RESULT_PROJECT $RESULT_DATASETS 
```

#### Step 6: Create a log router sink

The trigger event that we are interested in is `UpdateTag` and it is part of the the admin activity log, as described in [the product documentation](https://cloud.google.com/data-catalog/docs/how-to/audit-logging).  

Create a [log router sink](https://pantheon.corp.google.com/logs/router/sink) for the `UpdateTag` event as follows:

* Sink destination service: Pub/Sub topic
* Create a Pub/Sub topic and name it _pending_status_ (it must be in the same project as where your data assets reside)
* Use the inclusion filter: 

```
resource.type="audited_resource" protoPayload.serviceName="datacatalog.googleapis.com" resource.labels.method="google.cloud.datacatalog.v1.DataCatalog.UpdateTag"
protoPayload.request.tag.fields.data_product_status.enumValue.displayName="PENDING"
```


#### Step 7: Deploy the Cloud Function

The file [main.py](https://github.com/GoogleCloudPlatform/datacatalog-tag-engine/blob/main/examples/product_registration_pipeline/main.py) contains the source for the Cloud Function. The file [requirements.txt](https://github.com/GoogleCloudPlatform/datacatalog-tag-engine/blob/main/examples/product_registration_pipeline/requirements.txt). Update the _BUCKET_ and TAG_ENGINE_URL variables in main.py on lines 18 and 19, respectively. The _BUCKET_ variable should point to the location where you uploaded the config files in Step 2. 

Create the Cloud Function as follows:

* Environment: 1st gen
* Trigger type: Cloud Pub/Sub and choose _pending_status_
* Authentication: require authentication
* Entry point: _process_pending_status_event_

 
#### Step 9: Create the data product tags

```
curl -X POST $TAG_ENGINE_URL/static_asset_tags -d @examples/product_registration_pipeline/Finance/Trade_Transactions/data_product.json
curl -X POST $TAG_ENGINE_URL/static_asset_tags -d @examples/product_registration_pipeline/Customer/CRM/data_product.json
curl -X POST $TAG_ENGINE_URL/static_asset_tags -d @examples/product_registration_pipeline/Finance/Finwire_Archive/data_product.json
curl -X POST $TAG_ENGINE_URL/static_asset_tags -d @examples/product_registration_pipeline/Customer/Customer_Churn/data_product.json
```

#### Step 10: Manually update the status field of the data product tags
 
Go to the Data Catalog UI and find the Data Product Template from the Tag Template menu. 
Select <b>Search for entries using this template</b> and open one of the entries. 
From the entry's page, click on <b>Edit tag</b> to edit the data product tag attached to the
entry. Set the data product status to _PENDING_ and to save the tag to trigger the workflow.   

After a few minutes, click on the Entry List to look at the data assets associated with the product. 
Open one of the entries from the list. If the workflow succeeded, you should see a data resource tag 
at the table-level, a data sensitivity tag at the attribute level, and a data standardization at the  
attribute level. If you chose to generate policy tags, you should also see those appear next to each 
sensitive data tag. 


### Troubleshooting:

* Consult the Cloud Function logs if you do not see the expected output.<br> 
* Consult the App Engine logs if you encounter any errors while using Tag Engine:<br>
`gcloud app logs tail -s default`

